{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and declerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hassa\\\\Desktop\\\\Development\\\\Uni Projects\\\\Information Retrieval\\\\A1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import pprint\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import numpy as np\n",
    "import boolean\n",
    "import pyparsing\n",
    "from pyparsing import Word, alphas, oneOf, operatorPrecedence, opAssoc\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Vocabulary Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Break words like Veterans.Before, West.In amendment.Change\n",
    "\n",
    "def split_words(vocabl):\n",
    "    new_vocab = set()\n",
    "    for word in vocabl:\n",
    "        if re.search('^[a-zA-Z]+[.][a-zA-Z]+$',word) is not None:\n",
    "            print(re.search('^[a-zA-Z]+[.][a-zA-Z]+$',word))\n",
    "            w1, w2 = word.split('.')\n",
    "#             print(w1)\n",
    "# #             print(w2)\n",
    "            new_vocab.add(w1)\n",
    "            new_vocab.add(w2)\n",
    "        elif re.search('^[a-zA-Z]+[?][a-zA-Z]+$',word) is not None:\n",
    "# #             print(re.search('^[a-zA-Z]+[.][a-zA-Z]+$',word))\n",
    "            w1, w2 = word.split('?')\n",
    "# #             print(w1)\n",
    "# #             print(w2)\n",
    "            new_vocab.add(w1)\n",
    "            new_vocab.add(w2)\n",
    "        elif re.search('^[a-zA-Z]+[,][a-zA-Z]+$',word) is not None:\n",
    "# #             print(re.search('^[a-zA-Z]+[.][a-zA-Z]+$',word))\n",
    "            w1, w2 = word.split(',')\n",
    "# #             print(w1)\n",
    "# #             print(w2)\n",
    "            new_vocab.add(w1)\n",
    "            new_vocab.add(w2)\n",
    "        else:\n",
    "            new_vocab.add(word)\n",
    "    return new_vocab\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(word):\n",
    "    return word.translate(word.maketrans('','',string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "doc_contents = []\n",
    "printable = set(string.printable) \n",
    "# Printable characters are\n",
    "# 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stop_words = set()\n",
    "with open('Stopword-List.txt', 'r') as stop_word_file:\n",
    "    lines = stop_word_file.readlines()\n",
    "    for line in lines:\n",
    "        stop_words.add(line.split('\\n')[0])\n",
    "    stop_words.remove('')\n",
    "\n",
    "for file_number in range(0, 56):\n",
    "    with open(f'data/Trump Speechs/speech_{file_number}.txt', 'r') as file1:\n",
    "        lines = file1.readlines()\n",
    "#         print(f'File Number : speech_{file_number}.txt' )\n",
    "#         print(lines[0])\n",
    "        for line in lines:\n",
    "            doc_set = set()\n",
    "            # split words at . , whitespace ? ! : ;\n",
    "            for word in re.split('[.\\s,?!:;-]', line):\n",
    "                \n",
    "                \n",
    "                # Case Folding\n",
    "                word = word.lower()\n",
    "                \n",
    "                # Filter non-ASCII characters\n",
    "                word = ''.join(filter(lambda x: x in printable, word))\n",
    "                \n",
    "                # Remove Punctuations\n",
    "                word = remove_punctuation(word)\n",
    "                \n",
    "                if re.match('\\d+[A-Za-z]+',word):\n",
    "                    word = re.split('\\d+',word)[1]\n",
    "                if re.match('[A-Za-z]+\\d+',word):\n",
    "                    word = re.split('\\d+',word)[0]\n",
    "                \n",
    "                if len(word) == 0 or len(word) == 1 or word == '' or word == ' ':\n",
    "                    continue\n",
    "                if word in stop_words:\n",
    "                    continue\n",
    "                         \n",
    "                word = ps.stem(word)\n",
    "                    \n",
    "                vocab.add(word)\n",
    "                \n",
    "                doc_set.add(word)\n",
    "        \n",
    "        doc_contents.append(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size \n",
      "4850\n",
      "Total Number of Documents \n",
      "56\n",
      "{'acid', 'fast', 'ronald', 'achiev', 'flush', 'which', 'reagan', 'other', 'show', 'fulli', 'how', 'been', 'begin', 'battalion', 'back', 'there', 'libya', 'identifi', 'here', 'rethink', 'befor', 'famili', 'bill', 'took', 'increasingli', 'past', 'bureaucraci', 'offens', 'fold', 'simpli', 'infrastructur', 'men', 'turn', 'creat', '150', 'work', 'screen', 'midst', 'uniti', 'allow', 'requir', 'out', 'japan', 'civilian', 'secular', 'unheard', 'total', 'almost', 'system', 'now', 'with', 'refuge', 'clinton', 'heavili', 'technolog', 'nuclear', 'pay', 'short', 'dollar', 'disrupt', 'import', 'program', 'secretari', 'seem', 'research', 'substanti', 'combat', 'endless', 'navi', 'base', 'present', 'dead', 'ideolog', 'and', 'goal', 'centuri', 'societi', 'essenti', 'facil', 'yet', 'offset', 'spend', 'hard', 'path', 'relev', 'gradual', 'financi', 'suez', '540', 'did', 'through', 'agreement', '52', 'ambassador', 'design', 'when', 'terrorist', 'build', 'earn', 'whole', 'review', 'experi', 'chao', 'capabl', 'peopl', 'ever', 'need', 'classif', 'washington', 'sometim', 'retir', 'failur', 'feder', 'toward', 'prioriti', 'if', 'crisi', 'inflat', 'mani', 'invit', 'of', 'racial', 'off', 'much', '220', 'promot', '1915', '31', 'temper', 'administr', 'america', 'proudli', 'corp', 'fbi', 'known', 'sale', 'remov', 'hundr', 'trillion', 'sanction', 'proud', 'chief', 'admit', 'from', 'allianc', 'automat', 'iran', 'attrit', 'exceed', 'barack', 'unfit', 'instead', 'sever', 'histori', 'tax', 'powerhous', 'great', 'north', 'know', 'pillar', 'will', 'yesterday', 'sinc', 'process', 'fail', 'percent', 'energi', 'bring', 'deal', '548', '30', 'first', 'apolog', 'destroy', 'futur', 'fighter', '320', 'sinai', 'reduc', '308', 'invad', 'dispos', 'it', 'come', 'expand', 'sworn', '256', 'vital', 'record', '592', 'onli', 'st', 'same', 'brightest', 'number', 'brave', 'so', 'ago', 'interest', 'civil', 'afghanistan', 'by', 'im', 'minimum', 'common', 'is', 'obstruct', 'annual', 'cost', 'flag', 'stop', 'saudi', 'said', 'develop', 'new', 'adversari', 'west', 'releas', 'avoid', 'forward', 'propos', 'year', 'citi', 'our', 'couldnt', 'million', '1947', 'turmoil', 'defens', 'way', 'deter', 'cover', 'payment', 'third', 'could', 'examin', 'their', 'disastr', 'figur', 'submit', 'reform', 'territori', 'than', '276', 'repeatedli', 'event', 'share', 'server', 'control', 'note', 'privat', 'uniform', 'forc', 'smallest', 'fill', 'suffer', 'promptli', 'trump', 'union', 'valu', '202', 'offici', 'secur', 'near', 'speech', 'endors', 'action', 'top', '39', 'radic', '200', 'love', 'signific', 'major', 'warfar', 'claim', 'involv', 'return', 'put', 'missil', 'salut', 'democraci', 'workforc', 'unstabl', 'togeth', 'becom', 'internet', 'depart', 'weapon', 'realism', 'whi', 'even', '490', 'putin', 'money', 'more', 'veri', 'loos', 'unquest', 'former', 'china', 'sequest', 'accomplish', 'anoth', 'ii', 'occas', 'real', 'strategi', 'toppl', 'execut', 'fact', 'shrink', 'heighten', 'live', 'disqualifi', 'stabl', 'group', 'overseen', 'budget', 'recommend', 'father', 'you', 'averag', 'one', 'under', 'asia', 'wa', 'win', 'member', 'chang', 'disarray', 'unit', 'inform', 'benefit', 'right', 'billion', 'cold', 'face', 'or', 'then', 'tension', 'illeg', 'field', 'inclus', 'rapidli', 'request', 'he', 'effort', 'extinguish', 'never', 'trim', 'guid', 'decis', 'threat', 'degrad', 'spread', 'phone', 'foothold', 'islam', 'rule', 'surfac', '135', 'down', '10', 'sixth', 'sens', 'play', 'soon', 'also', 'thing', '2009', 'say', 'done', 'mean', 'stabil', 'admir', 'us', 'grid', 'against', 'procur', 'left', 'receiv', 'high', 'half', '27', 'danger', 'end', 'hamstr', 'non', 'american', 'ani', 'whether', 'hack', 'eas', 'spur', 'crucial', 'serv', 'cut', 'cant', 'should', 'alli', 'were', 'plan', 'troopswhich', 'activ', 'advanc', 'ransom', 'moment', 'air', 'key', 'again', 'well', 'save', 'expert', 'includ', 'trigger', 'letter', 'improp', 'troop', 'respect', 'smaller', 'below', 'ballist', 'econom', 'destruct', 'potenti', 'unleash', 'today', 'noth', 'call', 'countri', 'after', 'hammer', 'product', 'both', 'power', 'who', 'threaten', 'militari', 'tremend', 'everi', 'young', '113', '450', 'iraq', 'commun', 'taught', 'extort', 'make', 'reckless', 'conflict', 'hillari', 'leader', 'grow', 'public', 'less', 'addit', 'flew', 'heritag', 'caught', 'exempt', 'meet', 'duti', 'peac', 'foundat', 'most', 'do', 'wors', 'marin', 'plane', 'russia', 'them', 'wasnt', 'cash', 'spent', 'bleachbit', 'command', 'count', 'tomorrow', 'offic', 'produc', 'region', 'servic', 'dure', 'deep', 'care', 'per', 'choke', 'gain', 'estim', 'submarin', 'team', 'violenc', 'defend', 'etern', 'job', 'terror', 'strength', 'scandal', 'thousand', 'recal', 'talk', 'what', 'armi', 'homeland', 'track', 'embrac', 'reli', 'enforc', 'cybersecur', 'that', 'certainti', 'get', '553', 'vulner', '285', 'protect', 'rebuild', 'shown', 'but', 'brigad', 'help', 'arabia', 'term', 'follow', 'word', 'just', 'isi', 'partnership', 'soldier', 'old', 'domin', 'true', 'recruit', 'part', '22', 'amount', '182', '1940', 'long', 'ensur', 'let', 'bomber', 'healthcar', 'author', 'portion', 'displac', 'three', 'use', 'grown', 'medic', 'art', 'object', 'middl', 'integr', 'egypt', 'waterway', 'diplomaci', 'secret', 'tool', 'consid', 'run', 'direct', 'obama', 'form', 'korea', 'sector', 'support', 'week', 'adjust', 'wash', 'educ', 'greatest', 'conduct', 'regim', 'experienc', 'belliger', '000', 'across', 'cruiser', 'refus', 'last', 'some', 'ground', '554', 'meanwhil', 'thi', 'cyber', 'revenu', 'invest', 'employe', 'discard', 'strong', 'prevent', '385', 'emphas', 'god', 'tour', 'confidenti', 'though', 'respons', 'oper', 'south', 'classifi', 'into', 'seek', 'world', 'subpoena', 'germani', 'staff', 'incom', 'enemi', 'center', 'lowest', 'foreign', 'blackmail', 'life', 'aggress', 'they', 'campaign', 'fifti', 'must', 'final', 'prosper', 'handl', 'accord', 'institut', 'friendship', 'focus', 'gdp', 'rang', 'close', '350', 'size', 'sudden', 'time', 'prepar', 'warfight', 'war', 'vacuum', 'didnt', 'relat', 'exampl', 'mission', 'largest', 'day', 'best', 'ship', 'ask', 'sound', 'reduct', 'gimmick', 'earli', 'happi', 'immigr', 'immedi', 'start', 'creation', '23', 'europ', 'unlik', 'plu', 'not', 'cite', 'provid', 'least', 'got', 'modern', 'women', 'interven', 'readi', 'email', 'increas', 'replac', 'legaci', '2010', 'current', 'those', 'take', 'like', 'abil', 'safe', 'improv', 'age', 'thorough', 'desert', 'debt', 'core', 'wide', 'east', 'canal', 'tri', 'polici', 'among', 'larg', 'congress', 'look', 'wast', 'want', 'defi', 'worst', 'either', 'everywher', 'within', 'oppon', 'leaner', 'unpaid', 'me', 'be', 'govern', 'nation', 'my', '13', 'price', 'nato', 'state', 'she', 'level', 'train', 'about', 'veteran', '88', 'presid', 'around', 'gener', 'would', 'abl', 'rememb', 'lost', 'sponsor', '479', 'bipartisan', 'justic', 'expir', 'aircraft', 'line', 'defeat', 'divis', 'panel', 'laid', 'easili', 'over', 'joint', 'these', 'friend', 'fall', 'economi', 'inner', '36', 'law', 'ruin', 'nd', 'fund', 'conting', 'such', 'syria', 'an', 'elimin', 'report'}\n"
     ]
    }
   ],
   "source": [
    "print('Total Vocabulary Size ')\n",
    "print(len(vocab))\n",
    "print('Total Number of Documents ')\n",
    "print(len(doc_contents))\n",
    "print(doc_contents[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# print(sorted(list(vocab)))\n",
    "# for index,doc in enumerate(doc_contents):\n",
    "#     print('Vocab size of doc' + str(index))\n",
    "#     print(len(doc))\n",
    "\n",
    "vocab_list = sorted(list(vocab))\n",
    "\n",
    "term_doc_matrix_np = np.zeros((len(vocab), len(doc_contents)))\n",
    "\n",
    "for word_index, word in enumerate(vocab_list):\n",
    "    word_row = []\n",
    "    for doc_index, doc in enumerate(doc_contents):\n",
    "        if word in doc:\n",
    "            term_doc_matrix_np[word_index, doc_index] = 1\n",
    "        else:\n",
    "            term_doc_matrix_np[word_index, doc_index] = 0\n",
    "            \n",
    "print(term_doc_matrix_np)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('pickled/vocab.p', 'ab') as vocab_file:\n",
    "    pickle.dump(vocab, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickled/vocab.p', 'rb') as vocab_file:\n",
    "    vocabf = pickle.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query : running\n",
      "[1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [27]\n",
      " [28]\n",
      " [30]\n",
      " [32]\n",
      " [33]\n",
      " [34]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [44]\n",
      " [45]\n",
      " [46]\n",
      " [47]\n",
      " [50]\n",
      " [51]\n",
      " [52]\n",
      " [53]]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter your query : ')\n",
    "query = ps.stem(query)\n",
    "query_actions = []\n",
    "query_wanted = []\n",
    "if query in vocab:\n",
    "    term_index = vocab_list.index(query)\n",
    "    term_row = term_doc_matrix_np[term_index]\n",
    "    print(term_row)\n",
    "    doc_ids = np.argwhere(term_row == 1)\n",
    "    print(doc_ids)\n",
    "\n",
    "else:\n",
    "    print(f'{query} not present in vocabulary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 9), match='000dollar'>\n",
      "['', 'dollar']\n"
     ]
    }
   ],
   "source": [
    "test = '000dollar'\n",
    "x = re.match('\\d+[A-Za-z]+',test)\n",
    "print(x)\n",
    "x = re.split('\\d+',test)\n",
    "print(x)\n",
    "# For matching queries like\n",
    "# not hammer or pakistan\n",
    "# (magnum or not hammer) or not (polish and pakistan)\n",
    "x = re.match('(not)?\\s*(\\w+|(\\((not)?\\s*(\\w+)\\s+(and|or)\\s+(not)?\\s*(\\w+)\\)))\\s+(or|and)\\s+(not)?\\s*(\\w+|(\\((not)?\\s*(\\w+)\\s+(and|or)\\s+(not)?\\s*(\\w+)\\)))')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {'0', '1', '10', '11', '12', '16', '17', '18', '19', '2', '20', '21', '22', '24', '25', '26', '27', '28', '3', '30', '32', '33', '34', '35', '36', '37', '39', '4', '40', '41', '44', '45', '46', '47', '5', '50', '51', '52', '53', '6', '8', '9'}\n",
    "ans2 =set([str(x[0]) for x in doc_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "{'22', '1', '53', '25', '37', '8', '24', '10', '0', '39', '50', '17', '35', '40', '21', '33', '27', '52', '3', '36', '20', '16', '26', '45', '9', '30', '2', '5', '12', '19', '4', '32', '28', '46', '18', '47', '41', '34', '11', '51', '6', '44'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(len(ans))\n",
    "print((ans2))\n",
    "print(ans.difference(ans2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References :\n",
    "\n",
    "http://www.pyregex.com/\n",
    "http://cs231n.github.io/python-numpy-tutorial/\n",
    "\n",
    "https://www.online-utility.org/text/analyzer.jsp\n",
    "\n",
    "https://stackoverflow.com/questions/2118261/parse-boolean-arithmetic-including-parentheses-with-regex\n",
    "\n",
    "https://regex101.com/r/M8z3U4/1\n",
    "\n",
    "https://iq.opengenus.org/porter-stemmer/\n",
    "\n",
    "https://unnikked.ga/how-to-build-a-boolean-expression-evaluator-518e9e068a65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0\n",
      " 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0]\n",
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [12]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [19]\n",
      " [24]\n",
      " [26]\n",
      " [28]\n",
      " [29]\n",
      " [31]\n",
      " [37]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [42]\n",
      " [51]\n",
      " [53]\n",
      " [54]]\n",
      "25\n",
      "{'9', '19', '37', '53', '5', '42', '15', '2', '1', '26', '51', '24', '0', '39', '29', '31', '17', '41', '40', '28', '16', '12', '54', '3', '7'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "query_actions = []\n",
    "query_wanted = []\n",
    "term_index = vocab_list.index(ps.stem('actions'))\n",
    "term_row_actions = term_doc_matrix_np[term_index]\n",
    "\n",
    "query_actions = np.argwhere(term_row_actions == 1)\n",
    "term_index = vocab_list.index(ps.stem('wanted'))\n",
    "term_row_wanted = term_doc_matrix_np[term_index]\n",
    "\n",
    "query_wanted = np.argwhere(term_row_wanted == 1)\n",
    "\n",
    "and_query = np.array([1 if x == 1 and y == 1 else 0 for x,y in zip(term_row_actions, term_row_wanted)])\n",
    "and_doc_ids = np.argwhere(and_query == 1)\n",
    "print(and_query)\n",
    "print(and_doc_ids)\n",
    "\n",
    "ans = {'37', '3', '19', '1', '9', '40', '51', '16', '15', '12', '31', '41', '39', '0', '53', '26', '29', '17', '24', '54', '7', '2', '5', '28', '42'}\n",
    "ans2 =set([str(x[0]) for x in and_doc_ids])\n",
    "\n",
    "print(len(ans))\n",
    "print((ans2))\n",
    "print(ans.difference(ans2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['box']\n",
      "Current indexed Word : box\n",
      "box\n",
      "STATE\n",
      "['box']\n",
      "Outt\n",
      "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]\n",
      "deque([])\n",
      "{4, 9, 44, 45, 46, 47, 18, 50, 53, 54, 23, 25}\n"
     ]
    }
   ],
   "source": [
    "from collections import deque \n",
    "\n",
    "line = 'box'\n",
    "boperators = ['and', 'or']\n",
    "uoperators = ['not']\n",
    "  \n",
    "stack = deque() \n",
    "        \n",
    "def clean_word(word):\n",
    "    # Case Folding\n",
    "    word = word.lower()\n",
    "     # Filter non-ASCII characters\n",
    "    word = ''.join(filter(lambda x: x in printable, word))\n",
    "#     print(word)\n",
    "    # Remove Punctuations\n",
    "    if word != '(' and word != ')':\n",
    "        word = remove_punctuation(word)\n",
    "#     print(word)\n",
    "    if re.match('\\d+[A-Za-z]+',word):\n",
    "        word = re.split('\\d+',word)[1]\n",
    "    if re.match('[A-Za-z]+\\d+',word):\n",
    "        word = re.split('\\d+',word)[0]\n",
    "#     print(word)\n",
    "    word = ps.stem(word)\n",
    "#     print(word)\n",
    "    return word\n",
    "query = (re.split('[.\\s,?!:;-]', line))\n",
    "\n",
    "\n",
    "def evaluate_expression(index, query, stack, state):\n",
    "    \n",
    "    print(\"Current indexed Word : \" + str(query[index]))\n",
    "    result_query = []\n",
    "    \n",
    "    if query[index] == '(':\n",
    "        bracket_term = query[index] \n",
    "        result, new_index = evaluate_expression(index+1, query, stack, state)\n",
    "        if new_index == -1:\n",
    "            state = []\n",
    "        for x in range(index, new_index+1):\n",
    "            state.pop(x)\n",
    "            \n",
    "        print(\"Result\")\n",
    "        print(result)\n",
    "        index = new_index\n",
    "        stack.append({'state':True,'data':result, 'query': 'not' + not_of_term})\n",
    "        result_query = result\n",
    "        \n",
    "        print('new_index ' + str(new_index))\n",
    "        print(f'This should be a ) = {query[new_index]}')\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if query[index] in uoperators:\n",
    "        not_of_term = query[index] \n",
    "        result, new_index = evaluate_expression(index+1, query, stack, state)\n",
    "        if new_index == -1:\n",
    "            state = []\n",
    "        for x in range(0, new_index-index+1):\n",
    "            state.pop(x)\n",
    "            \n",
    "        print(\"Result\")\n",
    "        print(result)\n",
    "        index = new_index\n",
    "        not_result = [0 if int(x)==1 else 1 for x in result]\n",
    "        stack.append({'state':True,'data':not_result, 'query': 'not' + not_of_term})\n",
    "        result_query = not_result\n",
    "        \n",
    "    print(query[index])\n",
    "    if len(state) == 0:\n",
    "        return result_query, index\n",
    "    print('STATE')\n",
    "    print(state)\n",
    "    \n",
    "    \n",
    "    if (query[index] not in boperators) and (query[index] not in uoperators):\n",
    "        query[index] = ps.stem(query[index])\n",
    "        if query[index] not in vocab_list:\n",
    "            print(f'{query[index]} is not in vocabulary of index')\n",
    "            return [], -1\n",
    "        term_index = vocab_list.index(query[index])\n",
    "        term_row = term_doc_matrix_np[term_index]\n",
    "        stack.append({'state':True,'data':term_row,'query':query[index]})\n",
    "        result_query = term_row\n",
    "        index += 1\n",
    "        \n",
    "    if index >= len(query):\n",
    "        return stack.pop()['data'], -1\n",
    "    if len(state) == 0:\n",
    "        return result_query, index\n",
    "    \n",
    "    elif query[index] in boperators:\n",
    "#         query2 = clean_word(next_word)\n",
    "        \n",
    "#         next_word = query[index+1]\n",
    "#         print(next_word)\n",
    "#         if query2 not in vocab_list:\n",
    "#             print(f'{query2} is not in vocabulary of index')\n",
    "#             return [], -1\n",
    "        \n",
    "        \n",
    "#         term_index2 = vocab_list.index(query2)\n",
    "#         term_row2  = term_doc_matrix_np[term_index2]\n",
    "        \n",
    "        \n",
    "        term_row2, new_index = evaluate_expression(index+1, query, stack, state)\n",
    "        if new_index == -1:\n",
    "            state = []\n",
    "        for x in range(0, new_index-index+1):\n",
    "            state.pop(x)\n",
    "        \n",
    "        \n",
    "        query1 =  stack.pop()\n",
    "        term_row1 = []\n",
    "        \n",
    "        if query1['state'] == False:\n",
    "            term_index1 = vocab_list.index(query1['data'])\n",
    "            term_row1 = term_doc_matrix_np[term_index1]\n",
    "        \n",
    "        else:\n",
    "            term_row1 = query1['data']\n",
    "        \n",
    "        print(term_row1)\n",
    "        \n",
    "        print(term_row2)\n",
    "        result_query = []\n",
    "        if query[index] == 'and':\n",
    "            and_query = np.array([1 if int(x) == 1 and int(y) == 1 else 0 for x,y in zip(term_row1, term_row2)])\n",
    "            print(and_query)\n",
    "            and_doc_ids = np.argwhere(and_query == 1)\n",
    "            query_ans =  set([x[0] for x in and_doc_ids])\n",
    "            print(query_ans)\n",
    "            # stack.append({'state':True,'data':and_query,'query':query2})\n",
    "            result_query = and_query\n",
    "            \n",
    "        elif query[index] == 'or':\n",
    "            or_query = np.array([1 if int(x) == 1 or int(y) == 1 else 0 for x,y in zip(term_row1, term_row2)])\n",
    "            print('OR')\n",
    "            print(or_query)\n",
    "            or_doc_ids = np.argwhere(or_query == 1)\n",
    "            query_ans =  set([x[0] for x in or_doc_ids])\n",
    "            print(query_ans)\n",
    "            # stack.append({'state':True,'data':or_query,'query':query2})\n",
    "            result_query = or_query\n",
    "        index = new_index\n",
    "        \n",
    "    return result_query, index\n",
    "\n",
    "\n",
    "# for index, word in enumerate(query):  \n",
    "#     word = clean_word(word)\n",
    "#     print(word)\n",
    "\n",
    "query = [clean_word(word) for word in query ]\n",
    "print(query)\n",
    "ans, index = evaluate_expression(0, query, stack, query)\n",
    "\n",
    "print(\"Outt\")\n",
    "print(list(ans))\n",
    "\n",
    "print(stack)\n",
    "final_doc_ids = np.argwhere(np.array(ans) == 1)\n",
    "final_ans =  set([x[0] for x in final_doc_ids])\n",
    "print(final_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'47', '22', '13', '27', '7', '38', '25', '42', '15', '4', '33', '6', '8', '34', '30', '21', '43', '55', '44', '50', '10', '29', '31', '11', '49', '18', '46', '23', '52', '45', '32', '36', '14', '54', '35', '20', '48'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'9'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansq ={'18', '46', '4', '45', '50', '9', '47', '23', '54', '44', '25'}\n",
    "ansi = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55}\n",
    "ansq2 = set([str(x) for x in ansi])\n",
    "print(ansq2)\n",
    "ansq.difference(ansq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SPI - Simple Pascal Interpreter \"\"\"\n",
    "import re\n",
    "boperators = ['and', 'or']\n",
    "uoperators = ['not']\n",
    "\n",
    "# Token types\n",
    "#\n",
    "# EOF (end-of-file) token is used to indicate that\n",
    "# there is no more input left for lexical analysis\n",
    "INTEGER, PLUS, MINUS, MUL, DIV, LPAREN, RPAREN, EOF, TERM, AND, OR, NOT = (\n",
    "    'INTEGER', 'PLUS', 'MINUS', 'MUL', 'DIV', '(', ')', 'EOF', 'TERM', 'AND','OR', 'NOT'\n",
    ")\n",
    "\n",
    "\n",
    "class Token(object):\n",
    "    def __init__(self, type, value):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"String representation of the class instance.\n",
    "\n",
    "        Examples:\n",
    "            Token(TERM, Hello)\n",
    "            Token(AND, '&')\n",
    "            Token(NOT, '!')\n",
    "        \"\"\"\n",
    "        return 'Token({type}, {value})'.format(\n",
    "            type=self.type,\n",
    "            value=repr(self.value)\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class Lexer(object):\n",
    "    def __init__(self, text):\n",
    "        # client string input, e.g. \"hello | world & (why | are | you)\"\n",
    "        self.text = text\n",
    "        # self.pos is an index into self.text\n",
    "        self.pos = 0\n",
    "        self.current_char = self.text[self.pos]\n",
    "        \n",
    "\n",
    "    def error(self):\n",
    "        raise Exception('Invalid character')\n",
    "\n",
    "    def advance(self):\n",
    "        \"\"\"Advance the `pos` pointer and set the `current_char` variable.\"\"\"\n",
    "        self.pos += 1\n",
    "        if self.pos > len(self.text) - 1:\n",
    "            self.current_char = None  # Indicates end of input\n",
    "        else:\n",
    "            self.current_char = self.text[self.pos]\n",
    "\n",
    "    def skip_whitespace(self):\n",
    "        while self.current_char is not None and self.current_char.isspace():\n",
    "            self.advance()\n",
    "\n",
    "    def integer(self):\n",
    "        \"\"\"Return a (multidigit) integer consumed from the input.\"\"\"\n",
    "        result = ''\n",
    "        while self.current_char is not None and self.current_char.isdigit():\n",
    "            result += self.current_char\n",
    "            self.advance()\n",
    "        return int(result)\n",
    "    def word(self):\n",
    "        \"\"\"Return a (multidigit) integer consumed from the input.\"\"\"\n",
    "        result = ''\n",
    "        while self.current_char is not None and self.current_char.isalpha():\n",
    "            result += self.current_char\n",
    "            self.advance()\n",
    "        return str(result)\n",
    "\n",
    "    def get_next_token(self):\n",
    "        \"\"\"Lexical analyzer (also known as scanner or tokenizer)\n",
    "\n",
    "        This method is responsible for breaking a sentence\n",
    "        apart into tokens. One token at a time.\n",
    "        \"\"\"\n",
    "        while self.current_char is not None:\n",
    "\n",
    "            if self.current_char.isspace():\n",
    "                self.skip_whitespace()\n",
    "                continue\n",
    "\n",
    "            if self.current_char.isdigit():\n",
    "                return Token(INTEGER, self.integer())\n",
    "\n",
    "            \n",
    "            \n",
    "            if self.current_char == '&':\n",
    "#                 print('gotB' + self.current_char)\n",
    "                self.advance()\n",
    "                return Token(AND, 'AND')\n",
    "            \n",
    "            if self.current_char == '|':\n",
    "#                 print('gotB' + self.current_char)\n",
    "                self.advance()\n",
    "                return Token(OR, 'OR')\n",
    "            \n",
    "            if self.current_char == '!':\n",
    "#                 print('gotB' + self.current_char)\n",
    "                self.advance()\n",
    "                return Token(NOT,'NOT')\n",
    "            \n",
    "            if self.current_char == '+':\n",
    "                self.advance()\n",
    "                return Token(PLUS, '+')\n",
    "\n",
    "            if self.current_char == '-':\n",
    "                self.advance()\n",
    "                return Token(MINUS, '-')\n",
    "\n",
    "            if self.current_char == '*':\n",
    "                self.advance()\n",
    "                return Token(MUL, '*')\n",
    "\n",
    "            if self.current_char == '/':\n",
    "                self.advance()\n",
    "                return Token(DIV, '/')\n",
    "\n",
    "            if self.current_char == '(':\n",
    "                self.advance()\n",
    "                return Token(LPAREN, '(')\n",
    "\n",
    "            if self.current_char == ')':\n",
    "                self.advance()\n",
    "                return Token(RPAREN, ')')\n",
    "            \n",
    "            if self.current_char.isalpha():      \n",
    "#                 print('Got token  ' + self.current_char)\n",
    "                return Token(TERM, self.word())\n",
    "            \n",
    "            \n",
    "\n",
    "            self.error()\n",
    "\n",
    "        return Token(EOF, None)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                                                                             #\n",
    "#  PARSER                                                                     #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "\n",
    "class AST(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BinOp(AST):\n",
    "    def __init__(self, left, op, right):\n",
    "        self.left = left\n",
    "        self.token = self.op = op\n",
    "        self.right = right\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "        \n",
    "class Num(AST):\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.value = token.value\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        # set current token to the first token taken from the input\n",
    "        self.current_token = self.lexer.get_next_token()\n",
    "\n",
    "    def error(self):\n",
    "        raise Exception('Invalid syntax')\n",
    "\n",
    "    def eat(self, token_type):\n",
    "        # compare the current token type with the passed token\n",
    "        # type and if they match then \"eat\" the current token\n",
    "        # and assign the next token to the self.current_token,\n",
    "        # otherwise raise an exception.\n",
    "        if self.current_token.type == token_type:\n",
    "            self.current_token = self.lexer.get_next_token()\n",
    "        else:\n",
    "            self.error()\n",
    "\n",
    "    def factor(self):\n",
    "        \"\"\"factor : INTEGER | LPAREN expr RPAREN\"\"\"\n",
    "        token = self.current_token\n",
    "        \n",
    "        if token.type == INTEGER:\n",
    "            self.eat(INTEGER)\n",
    "            return Num(token)\n",
    "        \n",
    "        elif token.type == TERM:\n",
    "            self.eat(TERM)\n",
    "            return Num(token)\n",
    "        \n",
    "        elif token.type == NOT:\n",
    "            self.eat(NOT)\n",
    "            node = self.expr()\n",
    "            node.inverse = True\n",
    "            return node\n",
    "            \n",
    "        \n",
    "        \n",
    "        elif token.type == LPAREN:\n",
    "            self.eat(LPAREN)\n",
    "            node = self.expr()\n",
    "            self.eat(RPAREN)\n",
    "            return node\n",
    "\n",
    "    def term(self):\n",
    "        \"\"\"term : factor ((MUL | DIV) factor)*\"\"\"\n",
    "        node = self.factor()\n",
    "\n",
    "        while self.current_token.type in (MUL, DIV, AND):\n",
    "            token = self.current_token\n",
    "            if token.type == MUL:\n",
    "                self.eat(MUL)\n",
    "            elif token.type == DIV:\n",
    "                self.eat(DIV)\n",
    "            elif token.type == AND:\n",
    "                self.eat(AND)\n",
    "        \n",
    "            \n",
    "\n",
    "            node = BinOp(left=node, op=token, right=self.factor())\n",
    "\n",
    "        return node\n",
    "\n",
    "    def expr(self):\n",
    "        \"\"\"\n",
    "        expr   : term ((PLUS | MINUS) term)*\n",
    "        term   : factor ((MUL | DIV) factor)*\n",
    "        factor : INTEGER | LPAREN expr RPAREN\n",
    "        \"\"\"\n",
    "        node = self.term()\n",
    "\n",
    "        while self.current_token.type in (PLUS, MINUS, OR):\n",
    "            token = self.current_token\n",
    "            if token.type == OR:\n",
    "                self.eat(OR)\n",
    "            elif token.type == PLUS:\n",
    "                self.eat(PLUS)\n",
    "            elif token.type == MINUS:\n",
    "                self.eat(MINUS)\n",
    "            node = BinOp(left=node, op=token, right=self.term())\n",
    "\n",
    "        return node\n",
    "\n",
    "    def parse(self):\n",
    "        return self.expr()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                                                                             #\n",
    "#  INTERPRETER                                                                #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "\n",
    "class NodeVisitor(object):\n",
    "    def visit(self, node):\n",
    "        print('Checking Node Name')\n",
    "        print(type(node).__name__)\n",
    "        method_name = 'visit_' + type(node).__name__\n",
    "        visitor = getattr(self, method_name, self.generic_visit)\n",
    "        return visitor(node)\n",
    "\n",
    "    def generic_visit(self, node):\n",
    "        raise Exception('No visit_{} method'.format(type(node).__name__))\n",
    "\n",
    "\n",
    "class Interpreter(NodeVisitor):\n",
    "    def __init__(self, parser):\n",
    "        self.parser = parser\n",
    "\n",
    "    def visit_BinOp(self, node):\n",
    "        print(node)\n",
    "        if node.op.type == PLUS:\n",
    "            return str(self.visit(node.left)) + '+' +  str(self.visit(node.right))\n",
    "        elif node.op.type == MINUS:\n",
    "            return str(self.visit(node.left)) + '-' +  str(self.visit(node.right))\n",
    "        elif node.op.type == MUL:\n",
    "            return str(self.visit(node.left)) + '*' +  str(self.visit(node.right))\n",
    "        elif node.op.type == DIV:\n",
    "            return str(self.visit(node.left)) + '/' +  str(self.visit(node.right))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        elif node.op.type == AND:\n",
    "            left = self.visit(node.left)\n",
    "               \n",
    "            right = self.visit(node.right)\n",
    "            if left.inverse == True:\n",
    "                left.value = '!' + str(left.value)\n",
    "            if right.inverse == True:\n",
    "                right.value = '!' + str(right.value)\n",
    "            node.value = (left.value + ' AND ' +  right.value)\n",
    "            \n",
    "            return node\n",
    "        \n",
    "        elif node.op.type == OR:\n",
    "            left = self.visit(node.left)   \n",
    "            right = self.visit(node.right)\n",
    "            \n",
    "            if left.inverse == True:\n",
    "                left.value = '!' + str(left.value)\n",
    "            if right.inverse == True:\n",
    "                right.value = '!' + str(right.value)\n",
    "              \n",
    "            node.value = (left.value + ' OR ' +  right.value)\n",
    "            \n",
    "            return node\n",
    "        \n",
    "\n",
    "    def visit_Num(self, node):\n",
    "        term_index = vocab_list.index(ps.stem(node.value))\n",
    "        term_row = term_doc_matrix_np[term_index]\n",
    "        node.row = term_row\n",
    "        return node\n",
    "\n",
    "    def interpret(self):\n",
    "        tree = self.parser.parse()\n",
    "        return self.visit(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer = Lexer(text)\n",
    "parser = Parser(lexer)\n",
    "interpreter = Interpreter(parser)\n",
    "result = interpreter.interpret()\n",
    "print(result.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
