{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and declerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hassa\\\\Desktop\\\\Development\\\\Uni Projects\\\\Information Retrieval\\\\A1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import pprint\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import numpy as np\n",
    "import boolean\n",
    "import pyparsing\n",
    "from pyparsing import Word, alphas, oneOf, operatorPrecedence, opAssoc\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Vocabulary Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Break words like Veterans.Before, West.In amendment.Change\n",
    "\n",
    "def split_words(vocabl):\n",
    "    new_vocab = set()\n",
    "    for word in vocabl:\n",
    "        if re.search('^[a-zA-Z]+[.][a-zA-Z]+$',word) is not None:\n",
    "            print(re.search('^[a-zA-Z]+[.][a-zA-Z]+$',word))\n",
    "            w1, w2 = word.split('.')\n",
    "#             print(w1)\n",
    "# #             print(w2)\n",
    "            new_vocab.add(w1)\n",
    "            new_vocab.add(w2)\n",
    "        elif re.search('^[a-zA-Z]+[?][a-zA-Z]+$',word) is not None:\n",
    "# #             print(re.search('^[a-zA-Z]+[.][a-zA-Z]+$',word))\n",
    "            w1, w2 = word.split('?')\n",
    "# #             print(w1)\n",
    "# #             print(w2)\n",
    "            new_vocab.add(w1)\n",
    "            new_vocab.add(w2)\n",
    "        elif re.search('^[a-zA-Z]+[,][a-zA-Z]+$',word) is not None:\n",
    "# #             print(re.search('^[a-zA-Z]+[.][a-zA-Z]+$',word))\n",
    "            w1, w2 = word.split(',')\n",
    "# #             print(w1)\n",
    "# #             print(w2)\n",
    "            new_vocab.add(w1)\n",
    "            new_vocab.add(w2)\n",
    "        else:\n",
    "            new_vocab.add(word)\n",
    "    return new_vocab\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(word):\n",
    "    return word.translate(word.maketrans('','',string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "doc_contents = []\n",
    "printable = set(string.printable) \n",
    "# Printable characters are\n",
    "# 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stop_words = set()\n",
    "with open('Stopword-List.txt', 'r') as stop_word_file:\n",
    "    lines = stop_word_file.readlines()\n",
    "    for line in lines:\n",
    "        stop_words.add(line.split('\\n')[0])\n",
    "    stop_words.remove('')\n",
    "\n",
    "for file_number in range(0, 56):\n",
    "    with open(f'data/Trump Speechs/speech_{file_number}.txt', 'r') as file1:\n",
    "        lines = file1.readlines()\n",
    "#         print(f'File Number : speech_{file_number}.txt' )\n",
    "#         print(lines[0])\n",
    "        for line in lines:\n",
    "            doc_set = set()\n",
    "            # split words at . , whitespace ? ! : ;\n",
    "            for word in re.split('[.\\s,?!:;-]', line):\n",
    "                \n",
    "                \n",
    "                # Case Folding\n",
    "                word = word.lower()\n",
    "                \n",
    "                # Filter non-ASCII characters\n",
    "                word = ''.join(filter(lambda x: x in printable, word))\n",
    "                \n",
    "                # Remove Punctuations\n",
    "                word = remove_punctuation(word)\n",
    "                \n",
    "                if re.match('\\d+[A-Za-z]+',word):\n",
    "                    word = re.split('\\d+',word)[1]\n",
    "                if re.match('[A-Za-z]+\\d+',word):\n",
    "                    word = re.split('\\d+',word)[0]\n",
    "                \n",
    "                if len(word) == 0 or len(word) == 1 or word == '' or word == ' ':\n",
    "                    continue\n",
    "                if word in stop_words:\n",
    "                    continue\n",
    "                         \n",
    "                word = ps.stem(word)\n",
    "                    \n",
    "                vocab.add(word)\n",
    "                \n",
    "                doc_set.add(word)\n",
    "        \n",
    "        doc_contents.append(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size \n",
      "4850\n",
      "Total Number of Documents \n",
      "56\n",
      "{'refuge', 'civilian', 'averag', 'said', 'seem', 'unpaid', 'societi', 'ago', 'korea', 'sponsor', 'terrorist', 'shrink', 'week', 'waterway', 'near', '000', 'wasnt', 'worst', 'abil', 'enforc', 'racial', 'hillari', 'tax', 'abl', 'signific', 'count', '31', 'reagan', 'execut', 'build', 'warfight', 'immedi', 'so', 'million', 'onli', 'same', 'sometim', 'spend', 'action', 'review', 'common', 'speech', 'marin', 'ship', 'democraci', 'sequest', 'tomorrow', 'guid', 'defend', 'fast', 'unleash', 'didnt', 'medic', 'proudli', 'economi', 'benefit', 'run', 'direct', 'not', 'thing', 'divis', 'dead', 'get', 'rule', 'caught', 'with', 'terror', 'women', 'recommend', 'technolog', 'fbi', 'non', 'readi', 'largest', 'how', 'germani', 'were', 'against', 'exampl', 'expir', 'barack', 'look', 'level', 'arabia', 'support', 'potenti', 'return', 'libya', 'sound', 'that', 'depart', 'capabl', 'radic', 'down', 'disarray', 'substanti', 'be', 'command', 'note', 'danger', 'china', 'great', 'their', 'egypt', 'us', 'ruin', 'phone', 'accomplish', 'secret', 'play', 'rebuild', 'peac', 'reckless', 'aircraft', 'produc', '554', 'uniti', 'oppon', 'form', 'reli', 'brave', 'disastr', 'leaner', 'allianc', 'over', 'propos', 'secretari', 'if', 'threaten', 'through', 'oper', 'last', 'control', 'anoth', 'life', 'conduct', 'obstruct', 'desert', 'than', 'displac', 'classif', 'dollar', 'could', 'peopl', 'soon', 'interven', 'educ', 'money', 'extinguish', 'budget', 'flew', 'apolog', 'workforc', 'estim', '479', 'unit', 'conting', 'promptli', 'inner', 'power', 'turmoil', 'receiv', '202', 'off', 'iraq', 'product', 'record', 'claim', 'bipartisan', 'rethink', 'everi', 'dure', 'expert', 'sixth', 'pillar', 'tremend', 'or', 'examin', 'failur', '2009', 'hundr', 'internet', 'part', 'ballist', 'offici', 'want', '276', 'bureaucraci', 'did', 'improv', 'would', 'diplomaci', 'civil', 'wash', 'either', 'work', 'strong', 'most', 'long', 'call', '135', 'whole', 'larg', 'employe', 'more', 'ensur', 'admir', 'suffer', 'number', 'know', 'gimmick', 'heavili', 'tool', 'right', 'emphas', 'day', 'heritag', 'now', 'gain', 'reduc', '540', 'energi', 'econom', 'isi', 'is', 'them', 'revenu', 'toward', 'air', 'bomber', '10', 'exceed', 'stop', 'age', 'she', 'hard', 'easili', 'legaci', 'suez', 'unstabl', 'hack', 'payment', 'improp', 'north', 'an', 'instead', 'violenc', 'staff', 'destroy', 'plane', 'retir', 'sanction', 'plu', 'unheard', 'becom', 'fact', 'simpli', 'deal', 'defens', '592', 'soldier', 'minimum', 'short', 'friend', '256', 'moment', 'current', 'member', 'meet', '1915', 'ambassador', 'both', 'cut', 'strength', 'includ', 'befor', 'real', 'gradual', 'program', 'blackmail', 'asia', 'job', 'deep', 'top', '385', 'seek', 'extort', 'polici', 'ii', 'incom', '1947', 'realism', 'letter', 'cite', 'embrac', 'sever', 'like', 'close', 'essenti', 'billion', 'group', 'ani', 'experienc', 'back', 'under', 'futur', 'inclus', 'protect', 'term', 'vital', 'financi', 'powerhous', 'rang', 'lost', 'joint', 'provid', 'new', 'one', 'expand', 'face', 'what', 'say', 'line', 'islam', 'put', 'submit', 'bill', 'sworn', 'though', 'elimin', 'increasingli', 'around', '150', 'use', 'such', 'develop', 'center', 'mani', 'my', 'occas', 'size', 'safe', 'stabil', '450', 'sens', 'fighter', 'cant', 'half', 'illeg', 'histori', 'need', 'russia', 'washington', 'process', 'repeatedli', 'flag', 'also', 'decis', 'regim', 'vulner', 'into', 'cost', 'reform', 'three', 'automat', 'but', 'fulli', 'territori', '113', 'rememb', 'st', 'again', 'alli', '30', 'prioriti', 'experi', 'invit', 'uniform', 'iran', 'true', 'least', 'toppl', 'brightest', 'im', 'lowest', 'some', 'sudden', '308', 'recruit', 'cyber', 'grown', '200', 'portion', 'turn', 'tour', 'sale', 'scandal', 'year', 'he', 'total', 'begin', 'wa', 'after', 'admit', 'trigger', '220', 'modern', 'across', '320', 'justic', 'fill', 'defeat', 'invest', 'union', 'duti', 'less', 'forward', '27', 'identifi', 'import', 'choke', 'govern', 'of', 'way', 'europ', 'care', 'temper', 'start', 'tri', 'ransom', 'troop', 'chang', 'endless', 'love', '548', 'privat', 'destruct', 'object', 'nuclear', 'respect', 'unlik', 'ever', 'among', 'greatest', 'third', 'bleachbit', 'must', 'requir', 'chief', '39', 'serv', 'fifti', 'law', 'confidenti', 'feder', 'exempt', 'aggress', 'offens', 'ground', '88', 'acid', 'shown', 'integr', 'grow', 'best', 'even', 'congress', 'battalion', 'foundat', 'thi', 'other', 'weapon', 'our', 'loos', 'crucial', 'per', 'meanwhil', 'friendship', 'fund', 'system', 'activ', 'who', 'done', 'trim', 'taught', 'event', 'nato', 'research', 'young', 'today', 'thousand', 'saudi', 'troopswhich', 'brigad', 'key', 'screen', 'when', 'rapidli', 'citi', 'veri', 'disqualifi', 'obama', 'etern', 'and', 'navi', 'fold', 'administr', 'these', 'remov', 'those', 'domin', 'should', 'men', 'trillion', 'panel', 'facil', 'recal', 'take', 'wors', 'save', 'former', 'heighten', 'avoid', 'yet', 'ideolog', 'proud', 'train', 'within', 'south', 'reduct', 'left', 'server', 'nation', 'dispos', 'follow', 'been', 'sinai', 'end', 'below', 'homeland', '2010', '490', 'militari', 'core', 'overseen', 'share', 'present', 'time', 'hamstr', 'major', 'public', 'well', '36', 'inflat', 'forc', 'sector', 'salut', 'disrupt', 'father', 'enemi', 'god', 'commun', 'deter', 'base', 'american', 'adversari', 'nd', 'price', 'campaign', 'surfac', 'secular', 'east', 'prevent', 'almost', 'path', 'trump', 'procur', 'which', 'mean', 'grid', 'cold', 'subpoena', 'releas', 'email', 'promot', 'author', 'never', 'goal', 'spread', 'do', '553', 'you', 'out', 'focus', 'achiev', 'infrastructur', 'live', 'togeth', 'partnership', 'foothold', 'just', 'percent', 'spur', 'track', 'respons', 'hammer', 'creat', 'couldnt', 'old', 'much', 'ronald', 'then', 'thorough', 'japan', 'figur', 'got', 'institut', 'smallest', 'agreement', 'warfar', 'belliger', 'design', 'took', 'report', 'midst', 'inform', 'gdp', 'relev', 'corp', '285', '1940', 'will', '13', 'famili', 'past', 'chao', 'world', 'presid', 'increas', 'adjust', 'unquest', 'plan', 'art', 'bring', 'it', 'there', 'attrit', 'final', 'consid', 'addit', 'replac', 'threat', 'west', 'defi', 'cybersecur', 'help', 'debt', 'here', 'cash', 'relat', 'region', 'america', 'word', 'earn', 'combat', 'involv', 'fall', 'leader', 'prosper', 'war', 'servic', 'advanc', 'let', 'strategi', 'vacuum', 'invad', 'offset', 'they', 'missil', 'cruiser', 'creation', 'laid', 'effort', 'fail', 'conflict', 'request', 'putin', 'clinton', 'yesterday', 'crisi', 'make', 'amount', 'state', 'healthcar', '182', 'pay', 'allow', 'unfit', 'classifi', 'first', 'stabl', 'show', 'valu', 'come', 'mission', 'everywher', 'afghanistan', 'earli', 'by', 'spent', 'tension', 'submarin', '22', 'whether', 'offic', 'whi', 'immigr', 'high', 'talk', 'wast', 'noth', '350', 'prepar', 'foreign', 'happi', 'team', 'middl', 'smaller', 'gener', 'flush', 'interest', 'known', '52', 'canal', 'win', 'endors', 'wide', 'certainti', '23', 'about', 'ask', 'armi', 'veteran', 'degrad', 'centuri', 'field', 'me', 'accord', 'annual', 'syria', 'cover', 'discard', 'handl', 'countri', 'from', 'sinc', 'refus', 'eas', 'secur'}\n"
     ]
    }
   ],
   "source": [
    "print('Total Vocabulary Size ')\n",
    "print(len(vocab))\n",
    "print('Total Number of Documents ')\n",
    "print(len(doc_contents))\n",
    "print(doc_contents[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# print(sorted(list(vocab)))\n",
    "# for index,doc in enumerate(doc_contents):\n",
    "#     print('Vocab size of doc' + str(index))\n",
    "#     print(len(doc))\n",
    "\n",
    "vocab_list = sorted(list(vocab))\n",
    "\n",
    "term_doc_matrix_np = np.zeros((len(vocab), len(doc_contents)))\n",
    "\n",
    "for word_index, word in enumerate(vocab_list):\n",
    "    word_row = []\n",
    "    for doc_index, doc in enumerate(doc_contents):\n",
    "        if word in doc:\n",
    "            term_doc_matrix_np[word_index, doc_index] = 1\n",
    "        else:\n",
    "            term_doc_matrix_np[word_index, doc_index] = 0\n",
    "            \n",
    "print(term_doc_matrix_np)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('pickled/vocab.p', 'ab') as vocab_file:\n",
    "    pickle.dump(vocab, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickled/vocab.p', 'rb') as vocab_file:\n",
    "    vocabf = pickle.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query : running\n",
      "[1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [27]\n",
      " [28]\n",
      " [30]\n",
      " [32]\n",
      " [33]\n",
      " [34]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [44]\n",
      " [45]\n",
      " [46]\n",
      " [47]\n",
      " [50]\n",
      " [51]\n",
      " [52]\n",
      " [53]]\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter your query : ')\n",
    "query = ps.stem(query)\n",
    "query_actions = []\n",
    "query_wanted = []\n",
    "if query in vocab:\n",
    "    term_index = vocab_list.index(query)\n",
    "    term_row = term_doc_matrix_np[term_index]\n",
    "    print(term_row)\n",
    "    doc_ids = np.argwhere(term_row == 1)\n",
    "    print(doc_ids)\n",
    "\n",
    "else:\n",
    "    print(f'{query} not present in vocabulary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 9), match='000dollar'>\n",
      "['', 'dollar']\n"
     ]
    }
   ],
   "source": [
    "test = '000dollar'\n",
    "x = re.match('\\d+[A-Za-z]+',test)\n",
    "print(x)\n",
    "x = re.split('\\d+',test)\n",
    "print(x)\n",
    "# For matching queries like\n",
    "# not hammer or pakistan\n",
    "# (magnum or not hammer) or not (polish and pakistan)\n",
    "x = re.match('(not)?\\s*(\\w+|(\\((not)?\\s*(\\w+)\\s+(and|or)\\s+(not)?\\s*(\\w+)\\)))\\s+(or|and)\\s+(not)?\\s*(\\w+|(\\((not)?\\s*(\\w+)\\s+(and|or)\\s+(not)?\\s*(\\w+)\\)))')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {'0', '1', '10', '11', '12', '16', '17', '18', '19', '2', '20', '21', '22', '24', '25', '26', '27', '28', '3', '30', '32', '33', '34', '35', '36', '37', '39', '4', '40', '41', '44', '45', '46', '47', '5', '50', '51', '52', '53', '6', '8', '9'}\n",
    "ans2 =set([str(x[0]) for x in doc_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "{'22', '1', '53', '25', '37', '8', '24', '10', '0', '39', '50', '17', '35', '40', '21', '33', '27', '52', '3', '36', '20', '16', '26', '45', '9', '30', '2', '5', '12', '19', '4', '32', '28', '46', '18', '47', '41', '34', '11', '51', '6', '44'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(len(ans))\n",
    "print((ans2))\n",
    "print(ans.difference(ans2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References :\n",
    "\n",
    "http://www.pyregex.com/\n",
    "http://cs231n.github.io/python-numpy-tutorial/\n",
    "\n",
    "https://www.online-utility.org/text/analyzer.jsp\n",
    "\n",
    "https://stackoverflow.com/questions/2118261/parse-boolean-arithmetic-including-parentheses-with-regex\n",
    "\n",
    "https://regex101.com/r/M8z3U4/1\n",
    "\n",
    "https://iq.opengenus.org/porter-stemmer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0\n",
      " 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0]\n",
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [12]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [19]\n",
      " [24]\n",
      " [26]\n",
      " [28]\n",
      " [29]\n",
      " [31]\n",
      " [37]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [42]\n",
      " [51]\n",
      " [53]\n",
      " [54]]\n",
      "25\n",
      "{'1', '53', '37', '24', '0', '39', '17', '54', '7', '40', '3', '16', '26', '31', '9', '42', '2', '5', '15', '12', '19', '28', '41', '51', '29'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "query_actions = []\n",
    "query_wanted = []\n",
    "term_index = vocab_list.index(ps.stem('actions'))\n",
    "term_row_actions = term_doc_matrix_np[term_index]\n",
    "\n",
    "query_actions = np.argwhere(term_row_actions == 1)\n",
    "term_index = vocab_list.index(ps.stem('wanted'))\n",
    "term_row_wanted = term_doc_matrix_np[term_index]\n",
    "\n",
    "query_wanted = np.argwhere(term_row_wanted == 1)\n",
    "\n",
    "and_query = np.array([1 if x == 1 and y == 1 else 0 for x,y in zip(term_row_actions, term_row_wanted)])\n",
    "and_doc_ids = np.argwhere(and_query == 1)\n",
    "print(and_query)\n",
    "print(and_doc_ids)\n",
    "\n",
    "ans = {'37', '3', '19', '1', '9', '40', '51', '16', '15', '12', '31', '41', '39', '0', '53', '26', '29', '17', '24', '54', '7', '2', '5', '28', '42'}\n",
    "ans2 =set([str(x[0]) for x in and_doc_ids])\n",
    "\n",
    "print(len(ans))\n",
    "print((ans2))\n",
    "print(ans.difference(ans2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-e05fb0970a6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexpression1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'set' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pakistan', 'AND', 'Running']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"pakistan AND Running\"\n",
    "re.split('[\\s\\(\\)]', inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "the\n",
      "Institute\n",
      "institute\n",
      "institute\n",
      "institute\n",
      "institute\n",
      "institute\n",
      "institut\n",
      "for\n",
      "for\n",
      "for\n",
      "for\n",
      "for\n",
      "Energy\n",
      "energy\n",
      "energy\n",
      "energy\n",
      "energy\n",
      "energy\n",
      "energi\n",
      "Research\n",
      "research\n",
      "research\n",
      "research\n",
      "research\n",
      "research\n",
      "research\n",
      "cites\n",
      "cites\n",
      "cites\n",
      "cites\n",
      "cites\n",
      "cites\n",
      "cite\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "\"short\n",
      "\"short\n",
      "\"short\n",
      "short\n",
      "short\n",
      "short\n",
      "short\n",
      "run\"\n",
      "run\"\n",
      "run\"\n",
      "run\n",
      "run\n",
      "run\n",
      "run\n",
      "figure\n",
      "figure\n",
      "figure\n",
      "figure\n",
      "figure\n",
      "figure\n",
      "figur\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "of\n",
      "as\n",
      "as\n",
      "as\n",
      "as\n",
      "as\n",
      "much\n",
      "much\n",
      "much\n",
      "much\n",
      "much\n",
      "much\n",
      "much\n",
      "as\n",
      "as\n",
      "as\n",
      "as\n",
      "as\n",
      "$36\n",
      "$36\n",
      "$36\n",
      "36\n",
      "36\n",
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenizer Test\n",
    "line = 'the Institute for Energy Research cites a \"short-run\" figure of as much as $36'\n",
    "for word in re.split('[.\\s,?!:;-]', line):            \n",
    "    print(word)\n",
    "    # Case Folding\n",
    "    word = word.lower()\n",
    "    print(word)\n",
    "    # Filter non-ASCII characters\n",
    "    word = ''.join(filter(lambda x: x in printable, word))\n",
    "    print(word)\n",
    "    # Remove Punctuations\n",
    "    word = remove_punctuation(word)\n",
    "    print(word)\n",
    "    if re.match('\\d+[A-Za-z]+',word):\n",
    "        word = re.split('\\d+',word)[1]\n",
    "    if re.match('[A-Za-z]+\\d+',word):\n",
    "        word = re.split('\\d+',word)[0]\n",
    "    print(word)\n",
    "    if len(word) == 0 or len(word) == 1 or word == '' or word == ' ':\n",
    "        continue\n",
    "    if word in stop_words:\n",
    "        continue\n",
    "    print(word)\n",
    "    word = ps.stem(word)\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit\n",
      "or\n",
      "plane\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      "[1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55}\n"
     ]
    }
   ],
   "source": [
    "from collections import deque \n",
    "\n",
    "line = 'united OR plane'\n",
    "boperators = ['and', 'or']\n",
    "uoperators = ['not']\n",
    "  \n",
    "stack = deque() \n",
    "\n",
    "def clean_word(word):\n",
    "    # Case Folding\n",
    "    word = word.lower()\n",
    "     # Filter non-ASCII characters\n",
    "    word = ''.join(filter(lambda x: x in printable, word))\n",
    "#     print(word)\n",
    "    # Remove Punctuations\n",
    "    if word != '(' and word != ')':\n",
    "        word = remove_punctuation(word)\n",
    "#     print(word)\n",
    "    if re.match('\\d+[A-Za-z]+',word):\n",
    "        word = re.split('\\d+',word)[1]\n",
    "    if re.match('[A-Za-z]+\\d+',word):\n",
    "        word = re.split('\\d+',word)[0]\n",
    "#     print(word)\n",
    "    word = ps.stem(word)\n",
    "#     print(word)\n",
    "    return word\n",
    "query = iter(re.split('[.\\s,?!:;-]', line))\n",
    "\n",
    "for word in query:  \n",
    "    word = clean_word(word)\n",
    "    print(word)\n",
    "    if word not in boperators:\n",
    "        if word not in vocab_list:\n",
    "            print(f'{word} is not in vocabulary of index')\n",
    "            break\n",
    "        term_index = vocab_list.index(word)\n",
    "        term_row = term_doc_matrix_np[term_index]\n",
    "        \n",
    "        stack.append({'state':True,'data':term_row})\n",
    "    elif word in boperators:\n",
    "        next_word = next(query)\n",
    "        print(next_word)\n",
    "        \n",
    "        query1 =  stack.pop()\n",
    "        query2 = clean_word(next_word)\n",
    "        term_row1 = []\n",
    "        if query1['state'] == False:\n",
    "            term_index1 = vocab_list.index(query1['data'])\n",
    "            term_row1 = term_doc_matrix_np[term_index1]\n",
    "        \n",
    "        else:\n",
    "            term_row1 = query1['data']\n",
    "        \n",
    "        print(term_row1)\n",
    "        term_index2 = vocab_list.index(query2)\n",
    "        term_row2 = term_doc_matrix_np[term_index2]\n",
    "        print(term_row2)\n",
    "        if word == 'and':\n",
    "            and_query = np.array([1 if x == 1 and y == 1 else 0 for x,y in zip(term_row1, term_row2)])\n",
    "            print(and_query)\n",
    "            and_doc_ids = np.argwhere(and_query == 1)\n",
    "            query_ans =  set([x[0] for x in and_doc_ids])\n",
    "            print(query_ans)\n",
    "        elif word == 'or':\n",
    "            and_query = np.array([1 if x == 1 or y == 1 else 0 for x,y in zip(term_row1, term_row2)])\n",
    "            print(and_query)\n",
    "            and_doc_ids = np.argwhere(and_query == 1)\n",
    "            query_ans =  set([x[0] for x in and_doc_ids])\n",
    "            print(query_ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'22', '1', '25', '37', '8', '24', '10', '38', '0', '39', '50', '17', '13', '54', '7', '35', '40', '21', '33', '27', '52', '3', '36', '20', '16', '26', '31', '45', '9', '30', '49', '2', '5', '12', '19', '4', '55', '32', '28', '43', '44', '46', '18', '48', '47', '41', '34', '11', '51', '6', '29'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansq = {'31', '28', '50', '46', '37', '30', '54', '10', '18', '7', '1', '17', '41', '49', '6', '34', '36', '11', '45', '29', '26', '52', '13', '21', '24', '16', '25', '32', '33', '4', '44', '22', '8', '19', '40', '20', '38', '48', '0', '47', '27', '51', '43', '2', '35', '39', '9', '3', '5', '12', '55'}\n",
    "ansi = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55}\n",
    "ansq2 = set([str(x) for x in ansi])\n",
    "print(ansq2)\n",
    "ansq2.difference(ansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
